#########################################################################
##                      SERIF Queue Driver Mode                        ##
#########################################################################

SERIF "QueueDrivers" provides the ability to robustly process a stream
of documents using multiple SERIF processes running in parallel.  In
particular, each SERIF QueueDriver removes one document at a time from
an input queue, processes it, and writes the results to a output
queue.  Each QueueDriver continues to run, monitoring its source
location for new input documents, until it receives a signal to quit.

Individual QueueDrivers using the same input/output queues are
interchangable.  Thus, running multiple QueueDrivers with the same
queues provides a measure of redundancy.  In addition, a simple
watchdog process can be used to monitor the running QueueDrivers, and
restart any drivers that crash.

If a QueueDriver fails to process a document (either because an
exception was thrown while processing it, or because a QueueDriver
crashed), then the document will be moved to the end of the input
queue, where it will be tried again.  If the document fails again,
then it will be discarded.

Queue Configurations
~~~~~~~~~~~~~~~~~~~~
Depending on the CPU and memory resources that are available, you may
wish to consider several different queue configurations.  The basic
configuration, which uses a single input queue containing unprocessed
documents, and a single output queue containing fully processed
documents, is appropriate when you have enough memory available to run
the desired number of processes in parallel:

          Input Queue               Output Queue
          +---------+               +----------+
          |  doc1   | ---worker1--> | doc1.xml |
          |  doc2   |               | doc2.xml |
          |  doc3   | ---worker2--> | doc3.xml |
          |  ...    |               | ...      |
          |  docN   | ---worker3--> | docN.xml |
          +---------+               +----------+

In particular, on 64-bit Linux, you will need to reserve around 4GB
for each end-to-end English SERIF process.  However, if you have less
than 4GB/CPU available, then you may also want to consider setting up
multiple queues, with specialized workers:

    Input         Values         Parse           Output   
  +-------+      +-------+      +------+      +----------+
  | doc1  |      | doc1  | ---> | doc1 |      | doc1.xml |
  | doc2  |      | doc2  |      | doc2 | ---> | doc2.xml |
  | doc3  | ---> | doc3  | ---> | doc3 |      | doc3.xml |
  | ...   |      | ...   |      | ...  | ---> | ...      |
  | docN  |      | docN  | ---> | docN |      | docN.xml |
  +-------+      +-------+      +------+      +----------+

This example uses four queues and six workers: one worker running the
stages before parsing (~600MB memory), three workers running the parse
stage (~2GB each), and two workers running the remaining stages (~1GB
each).

Implementation
~~~~~~~~~~~~~~
SERIF currently provides a single QueueDriver implementation, using
disk-based queues.  (Implementations using other queuing mechanisms,
such as databases, could be added.)  Each queue corresponds to a
single directory, and queue items are encoded using files.  File
extensions are used to indicate the status of each queue item:

  <docid>.ready.............. queue item is ready to be processed

  <docid>.<N>.working........ queue item is currently being processed 
                              by QueueDriver <N> (as an input item).

  <docid>.<N>.writing.xml.... new queue item is being generated by 
                              QueueDriver <N>, but is not yet finished.

  <docid>.failed............. queue item was processed by a QueueDriver,
                              but failed.  It should be retried once
                              more, and discarded if it fails again.

Queue "locking" is accomplished using atomic file renaming [1].  For
example, when a QueueDriver wants to lock its input file before it
begins working on it, it does so by renaming it from "<docid>.ready"
to "<docid>.<N>.working".  The filenames that include QueueDriver ids
("<N>") are considered "locked" and should only be accessed by the
corresponding QueueDriver. 

Note that this means that if you wish to add files manually to an
input queue, then you should first copy them to that directory, and
then rename them to have the ".ready" extension -- if you write
directly to a file containing the ".ready" extension, then a
QueueDriver might read it before you finish writing it.  Similarly,
when reading the output queue directory, you should only read files
that have the ".ready" extension.

[1] Atomic file renaming is typically guaranteed on UNIX; but may or
    may not be guaranteed on windows, depending on the file system
    used.  The disk-based QueueDriver should only be used with 
    file systems that guarantee atomic file renaming.

